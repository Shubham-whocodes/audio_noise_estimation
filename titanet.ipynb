{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aee110c",
   "metadata": {},
   "source": [
    "# TitaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faefdb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.67.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch==1.13.0\n",
      "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.0) (4.12.2)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (75.3.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.45.0)\n",
      "Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchaudio==0.13.0\n",
      "  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: torch==1.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchaudio==0.13.0) (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.0->torchaudio==0.13.0) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.0->torchaudio==0.13.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.0->torchaudio==0.13.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.0->torchaudio==0.13.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==1.13.0->torchaudio==0.13.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchaudio==0.13.0) (75.3.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchaudio==0.13.0) (0.45.0)\n",
      "Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.2\n",
      "    Uninstalling torchaudio-2.2.2:\n",
      "      Successfully uninstalled torchaudio-2.2.2\n",
      "Successfully installed torchaudio-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas==1.3.3\n",
      "  Downloading pandas-1.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==1.3.3) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==1.3.3) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==1.3.3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas==1.3.3) (1.16.0)\n",
      "Downloading pandas-1.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-1.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting librosa==0.8.1\n",
      "  Downloading librosa-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting audioread>=2.0.0 (from librosa==0.8.1)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (1.4.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (5.1.1)\n",
      "Collecting resampy>=0.2.2 (from librosa==0.8.1)\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numba>=0.43.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (0.60.0)\n",
      "Collecting soundfile>=0.10.2 (from librosa==0.8.1)\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
      "Collecting pooch>=1.0 (from librosa==0.8.1)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa==0.8.1) (21.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from numba>=0.43.0->librosa==0.8.1) (0.43.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->librosa==0.8.1) (3.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.8.1) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.8.1) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.1) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from soundfile>=0.10.2->librosa==0.8.1) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.8.1) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.1) (2024.8.30)\n",
      "Downloading librosa-0.8.1-py3-none-any.whl (203 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: audioread, soundfile, resampy, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.8.1 pooch-1.8.2 resampy-0.4.3 soundfile-0.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.9.2)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (75.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Installing collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.19.2 setproctitle-1.3.4 wandb-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pathlib\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pathlib\n",
      "Successfully installed pathlib-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install torch==1.13.0\n",
    "%pip install torchaudio==0.13.0\n",
    "%pip install pandas==1.3.3\n",
    "%pip install librosa==0.8.1\n",
    "%pip install transformers\n",
    "%pip install wandb\n",
    "%pip install pathlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71539ab7",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87bdd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import logging\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import titanet_modules as modules\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88949b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "env: TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "%env TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env HF_DATASETS_CACHE=/home/ec2-user/SageMaker/cache\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbef0da",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b459d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NISQADataset(object):\n",
    "    def __init__(self,args,d_type):\n",
    "        self.args = args\n",
    "        self.d_type = d_type\n",
    "        self.dfdata = self.load_CSV_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dfdata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row_data = self.dfdata.iloc[idx]\n",
    "        spectrogram, mos = self.generate_waveform_spectrogram_mos_pair(row_data)\n",
    "        return spectrogram, mos\n",
    "         \n",
    "    def generate_spectrograms(self, waveform):\n",
    "        \n",
    "        spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
    "                                                sample_rate=self.args.sample_rate,\n",
    "                                                n_fft=self.args.n_fft,\n",
    "                                                win_length=self.args.win_length,\n",
    "                                                hop_length=self.args.hop_length,\n",
    "                                                n_mels=self.args.n_mels\n",
    "                                            )\n",
    "        \n",
    "        spectrogram = spectrogram_transform(waveform)\n",
    "        return spectrogram\n",
    "\n",
    "    def generate_waveform_spectrogram_mos_pair(self, row_data):  \n",
    "        audio_path = self.args.data_dir + '/' + self.d_type + '/' + row_data[self.args.csv_deg]\n",
    "        mos_rating = row_data[self.args.csv_mos_train]\n",
    "        \n",
    "        mos_rating = torch.tensor(mos_rating)\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_path) \n",
    "\n",
    "        resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)  \n",
    "        waveform = resample_transform(waveform)  \n",
    "\n",
    "        spectrogram = self.generate_spectrograms(waveform)\n",
    "        spectrogram = spectrogram.squeeze(0)\n",
    "            \n",
    "        return spectrogram,mos_rating\n",
    "        \n",
    "\n",
    "    def load_CSV_data(self):\n",
    "        data = pd.read_csv(self.args.titanet_csv_file + '/' + self.d_type +'.csv')       \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f652a13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    spectrograms = [item[0] for item in batch]\n",
    "    mos = [item[1] for item in batch]\n",
    "    \n",
    "    max_len = max([spectrogram.shape[-1] for spectrogram in spectrograms])\n",
    "    \n",
    "    padded_spectrograms = [F.pad(spectrogram, (0, max_len - spectrogram.shape[-1])) for spectrogram in spectrograms]\n",
    "    \n",
    "    padded_spectrograms = torch.stack(padded_spectrograms)\n",
    "    mos = torch.stack(mos)\n",
    "    \n",
    "    return padded_spectrograms, mos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4f1869-bc1b-40be-96a5-972e0ec5572f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def listnet_loss(y_i, z_i):\n",
    "    P_y_i = F.softmax(y_i.float(), dim=0)\n",
    "    P_z_i = F.softmax(z_i.float(), dim=0)\n",
    "    return - torch.sum(P_y_i * torch.log(P_z_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c2b38",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f4f639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TitaNetEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels,\n",
    "        n_mega_blocks,\n",
    "        n_sub_blocks,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        mega_block_kernel_size,\n",
    "        attention_hidden_size,\n",
    "        embedding_size,\n",
    "        prolog_kernel_size=3,\n",
    "        epilog_kernel_size=1,\n",
    "        se_reduction=16,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(TitaNetEncoder, self).__init__()\n",
    "\n",
    "        self.prolog = modules.ConvBlock1d(n_mels, hidden_size, prolog_kernel_size)\n",
    "        self.mega_blocks = nn.Sequential(\n",
    "            *[\n",
    "                MegaBlock(\n",
    "                    hidden_size,\n",
    "                    hidden_size,\n",
    "                    mega_block_kernel_size,\n",
    "                    n_sub_blocks,\n",
    "                    se_reduction=se_reduction,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for _ in range(n_mega_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.epilog = modules.ConvBlock1d(hidden_size, output_size, epilog_kernel_size)\n",
    "        self.pool = nn.Sequential(\n",
    "                AttentiveStatsPooling(output_size, attention_hidden_size),\n",
    "                # nn.BatchNorm1d(output_size * 2),\n",
    "            )\n",
    "\n",
    "        # self.linear = nn.Linear(output_size * 2, embedding_size)\n",
    "        self.linear = nn.Linear(output_size * 2, embedding_size)\n",
    "\n",
    "\n",
    "    def forward(self, spectrograms):\n",
    "        # [B, M, T] -> [B, H, T]\n",
    "        prolog_outputs = self.prolog(spectrograms)\n",
    "\n",
    "        # [B, H, T] -> [B, H, T]\n",
    "        mega_blocks_outputs = self.mega_blocks(prolog_outputs)\n",
    "\n",
    "        # [B, H, T] -> [B, DE, T]\n",
    "        encodings = self.epilog(mega_blocks_outputs)\n",
    "        \n",
    "        # [B, DE, T] -> [B, DE * 2]\n",
    "        pooled = self.pool(encodings)\n",
    "\n",
    "        # [B, DE * 2] -> [B, E]\n",
    "        # return torch.clamp(self.linear(pooled), min=0, max=5)\n",
    "        return self.linear(pooled)\n",
    "\n",
    "\n",
    "class MegaBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        kernel_size,\n",
    "        n_sub_blocks,\n",
    "        se_reduction=16,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(MegaBlock, self).__init__()\n",
    "\n",
    "        # Store attributes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define sub-blocks composed of depthwise convolutions\n",
    "        channels = [input_size] + [output_size] * n_sub_blocks\n",
    "        self.sub_blocks = nn.Sequential(\n",
    "            *[\n",
    "                modules.ConvBlock1d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    activation=\"relu\",\n",
    "                    dropout=dropout,\n",
    "                    depthwise=True,\n",
    "                )\n",
    "                for in_channels, out_channels in zip(channels[:-1], channels[1:])\n",
    "            ],\n",
    "            modules.SqueezeExcitation(output_size, reduction=se_reduction)\n",
    "        )\n",
    "\n",
    "        # Define the final skip connection\n",
    "        self.skip_connection = nn.Sequential(\n",
    "            nn.Conv1d(input_size, output_size, kernel_size=1),\n",
    "            nn.BatchNorm1d(output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, prolog_outputs):\n",
    "        \"\"\"\n",
    "        Given prolog outputs of shape [B, H, T], return\n",
    "        a feature tensor of shape [B, H, T]\n",
    "\n",
    "        B: batch size\n",
    "        H: hidden size\n",
    "        T: maximum number of time steps (frames)\n",
    "        \"\"\"\n",
    "        # [B, H, T] -> [B, H, T]\n",
    "        mega_block_outputs = self.skip_connection(prolog_outputs) + self.sub_blocks(\n",
    "            prolog_outputs\n",
    "        )\n",
    "        return F.dropout(\n",
    "            F.relu(mega_block_outputs), p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "class AttentiveStatsPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, eps=1e-6):\n",
    "        super(AttentiveStatsPooling, self).__init__()\n",
    "\n",
    "        # Store attributes\n",
    "        self.eps = eps\n",
    "\n",
    "        # Define architecture\n",
    "        self.in_linear = nn.Linear(input_size, hidden_size)\n",
    "        self.out_linear = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        \"\"\"\n",
    "        Given encoder outputs of shape [B, DE, T], return\n",
    "        pooled outputs of shape [B, DE * 2]\n",
    "\n",
    "        B: batch size\n",
    "        T: maximum number of time steps (frames)\n",
    "        DE: encoding output size\n",
    "        \"\"\"\n",
    "        # Compute a scalar score for each frame-level feature\n",
    "        # [B, DE, T] -> [B, DE, T]\n",
    "        energies = self.out_linear(\n",
    "            torch.tanh(self.in_linear(encodings.transpose(1, 2)))\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # Normalize scores over all frames by a softmax function\n",
    "        # [B, DE, T] -> [B, DE, T]\n",
    "        alphas = torch.softmax(energies, dim=2)\n",
    "\n",
    "        # Compute mean vector weighted by normalized scores\n",
    "        # [B, DE, T] -> [B, DE]\n",
    "        means = torch.sum(alphas * encodings, dim=2)\n",
    "\n",
    "        # Compute std vector weighted by normalized scores\n",
    "        # [B, DE, T] -> [B, DE]\n",
    "        residuals = torch.sum(alphas * encodings ** 2, dim=2) - means ** 2\n",
    "        stds = torch.sqrt(residuals.clamp(min=self.eps))\n",
    "\n",
    "        # Concatenate mean and std vectors to produce\n",
    "        # utterance-level features\n",
    "        # [[B, DE]; [B, DE]] -> [B, DE * 2]\n",
    "        return torch.cat([means, stds], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937ed4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f98ff4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accc519",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a377fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = '/home/ec2-user/SageMaker/Noise_modelling/config.yaml'\n",
    "\n",
    "with open(config_file, \"r\") as ymlfile:\n",
    "    config_dict = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "args = Config(**config_dict)\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8216dda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = NISQADataset(args,'train')\n",
    "dataset_val = NISQADataset(args,'val')\n",
    "dataset_test = NISQADataset(args,'test')\n",
    "\n",
    "\n",
    "train_reader = DataLoader(dataset_train,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                collate_fn=custom_collate_fn,\n",
    "                drop_last=True,\n",
    "                )\n",
    "\n",
    "val_reader = DataLoader(dataset_val,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                collate_fn=custom_collate_fn,\n",
    "                persistent_workers=True,\n",
    "                drop_last=True,\n",
    "                )\n",
    "\n",
    "test_reader = DataLoader(dataset_test,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                collate_fn=custom_collate_fn,\n",
    "                persistent_workers=True,\n",
    "                drop_last=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b22cd8f-d5c8-462a-a68f-58962623999d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_test.dfdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d801d2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TitaNetEncoder(\n",
    "            args.n_mels,\n",
    "            args.n_mega_blocks,\n",
    "            args.n_sub_blocks,\n",
    "            args.encoder_hidden_size,\n",
    "            args.encoder_output_size,\n",
    "            args.mega_block_kernel_size,\n",
    "            args.attention_hidden_size,\n",
    "            args.embedding_size,\n",
    "            prolog_kernel_size=args.prolog_kernel_size,\n",
    "            epilog_kernel_size=args.epilog_kernel_size,\n",
    "            se_reduction=args.se_reduction,\n",
    "            dropout=args.dropout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1156d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5637fdaa-e55d-4435-85af-056c7b71de9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 0.89M parameters\n"
     ]
    }
   ],
   "source": [
    "div=1e6\n",
    "n_params = sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad]) / div\n",
    "print(f\"This model has {n_params:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39f1e4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517784fd",
   "metadata": {},
   "source": [
    "# Eval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9c2a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_result(predicts, labels):\n",
    "    spearman_corr, _ = spearmanr(predicts, labels)\n",
    "    corr, _ = pearsonr(predicts, labels)\n",
    "    res = {\"Prearson Corr\":corr,\"Spearman Corr\":spearman_corr,\"Eval Loss\":np.sqrt(mean_squared_error(predicts, labels))}\n",
    "    return res\n",
    "\n",
    "def eval_model(model, validset_reader):\n",
    "    model.eval()\n",
    "    predicts = list()\n",
    "    labels = list()\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, mos in tqdm(validset_reader):\n",
    "            spectrograms, mos = spectrograms.to(device), mos.to(device)\n",
    "\n",
    "            score_tensor = mos.to(torch.float)\n",
    "            prob = model(spectrograms)\n",
    "            \n",
    "            predict = prob.type_as(score_tensor).view(-1).tolist()\n",
    "            score = score_tensor.view(-1).tolist()\n",
    "            predicts.extend(predict)\n",
    "            labels.extend(score)\n",
    "            \n",
    "        results = eval_result(predicts, labels)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93c46a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, args, trainset_reader, validset_reader,testset_reader):\n",
    "    saved_checkpoints = []\n",
    "    save_path = args.output_dir\n",
    "    best_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    gamma = 0.99999\n",
    "\n",
    "    total_steps = len(trainset_reader.dataset.dfdata)/args.batch_size\n",
    "\n",
    "    t_total = int(\n",
    "        total_steps / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        logger.info('Training Started !')\n",
    "        for spectrograms,mos in tqdm(trainset_reader):\n",
    "            spectrograms, mos = spectrograms.to(device), mos.to(device)\n",
    "\n",
    "            model.train()\n",
    "        \n",
    "            score = model(spectrograms)\n",
    "                    \n",
    "            pred_score = score.view(-1)\n",
    "            score_tensor = mos.view(-1)\n",
    "\n",
    "            loss = F.mse_loss(pred_score, score_tensor.to(torch.float))\n",
    "            \n",
    "#             lambda_ = 1/(1+math.exp(gamma*((int(args.num_train_epochs)/2)-epoch)))\n",
    "            \n",
    "#             loss = lambda_*loss + (1-lambda_)*listnet_loss(score_tensor,pred_score)\n",
    "       \n",
    "            running_loss += loss.item()\n",
    "            if args.gradient_accumulation_steps != 0:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "            loss.backward()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        logger.info('Epoch: {}, Loss: {} ,LR : {}'.format(epoch, (running_loss / global_step),scheduler.get_last_lr()[0]))\n",
    "        \n",
    "        train_res = {\n",
    "            \"Train Loss\":(running_loss / global_step),\n",
    "            \"Learning Rate\" : scheduler.get_last_lr()[0],\n",
    "        }\n",
    "                \n",
    "        logger.info('Eval Started ! ')\n",
    "        result_dict_val = eval_model(model, validset_reader)\n",
    "        result_dict_test = eval_model(model, testset_reader)\n",
    "        logger.info(result_dict_val)\n",
    "        \n",
    "        train_res.update({'val':result_dict_val})\n",
    "        train_res.update({'test':result_dict_test})\n",
    "        \n",
    "        wandb.log(train_res)\n",
    "\n",
    "        check_point_path = save_path + f\"/model_{epoch}_best.pt\"\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': model.state_dict()},check_point_path)\n",
    "        \n",
    "        saved_checkpoints.append(check_point_path)\n",
    "\n",
    "        if len(saved_checkpoints) > args.max_model_save:\n",
    "            old_checkpoint = saved_checkpoints.pop(0)\n",
    "            if os.path.exists(old_checkpoint):\n",
    "                os.remove(old_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdf4eb09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 17/89 [00:11<00:49,  1.46it/s]\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f4542321ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=args.wandb_key)\n",
    "wandb.init(project=args.wandb_proj_name, config=args, name=args.wandb_run_name)\n",
    "\n",
    "train_model(model, args, train_reader, val_reader, test_reader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cf085-0f94-4b5e-b28d-53abe2108457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f94db-2c80-4eda-9859-ce7cfd60bd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
